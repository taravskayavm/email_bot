"""Handlers for ingest flow powered by aiogram."""

from __future__ import annotations

import io
import re
from typing import Any, Iterable

from aiogram import F, Router, types
from aiogram.exceptions import TelegramBadRequest
from aiogram.types import CallbackQuery
from aiogram.utils.keyboard import InlineKeyboardBuilder
from aiogram.utils.markdown import hcode

from emailbot.messaging_utils import is_blocked, is_suppressed
from emailbot.pipelines.ingest import ingest_emails
from emailbot.pipelines.ingest_url import ingest_url
from emailbot.settings import resolve_label
from emailbot.utils.file_email_extractor import ExtractError, extract_emails_from_bytes
from emailbot.ui.messages import format_parse_summary

router = Router()
URL_RE = re.compile(r"""(?ix)\b((?:https?://)?(?:www\.)?[^\s<>()]+?\.[^\s<>()]{2,}[^\s<>()]*)(?=$|[\s,;:!?)}\]])""")
REJECT_LABELS = {
    "no_at_sign": "Ð½ÐµÑ‚ ÑÐ¸Ð¼Ð²Ð¾Ð»Ð° @",
    "empty_local_or_domain": "Ð¿ÑƒÑÑ‚Ð°Ñ Ð»Ð¾ÐºÐ°Ð»ÑŒ/Ð´Ð¾Ð¼ÐµÐ½",
    "local_not_ascii": "Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð°Ñ Ñ‡Ð°ÑÑ‚ÑŒ Ð½Ðµ ASCII",
    "local_edge_dot": "Ñ‚Ð¾Ñ‡ÐºÐ° Ð² Ð½Ð°Ñ‡Ð°Ð»Ðµ/ÐºÐ¾Ð½Ñ†Ðµ Ð»Ð¾ÐºÐ°Ð»Ð¸",
    "local_consecutive_dots": "Ð´Ð²Ðµ Ñ‚Ð¾Ñ‡ÐºÐ¸ Ð¿Ð¾Ð´Ñ€ÑÐ´ Ð² Ð»Ð¾ÐºÐ°Ð»Ð¸",
    "local_bad_chars": "Ð½ÐµÐ´Ð¾Ð¿ÑƒÑÑ‚Ð¸Ð¼Ñ‹Ðµ ÑÐ¸Ð¼Ð²Ð¾Ð»Ñ‹ Ð² Ð»Ð¾ÐºÐ°Ð»Ð¸",
    "domain_bad_shape": "Ð½ÐµÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ð¹ Ð´Ð¾Ð¼ÐµÐ½",
    "domain_idna_fail": "Ð¾ÑˆÐ¸Ð±ÐºÐ° IDNA-ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð´Ð¾Ð¼ÐµÐ½Ð°",
    "domain_too_long": "ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ð¹ Ð´Ð¾Ð¼ÐµÐ½",
    "domain_label_size": "Ð´Ð»Ð¸Ð½Ð° Ð»ÐµÐ¹Ð±Ð»Ð° Ð´Ð¾Ð¼ÐµÐ½Ð° Ð½ÐµÐ²ÐµÑ€Ð½Ð°",
    "domain_label_dash": "Ð»ÐµÐ¹Ð±Ð» Ð´Ð¾Ð¼ÐµÐ½Ð° Ð½Ð°Ñ‡Ð¸Ð½Ð°ÐµÑ‚ÑÑ/Ð·Ð°ÐºÐ°Ð½Ñ‡Ð¸Ð²Ð°ÐµÑ‚ÑÑ Ð´ÐµÑ„Ð¸ÑÐ¾Ð¼",
    "missing_dep_openpyxl": "Ð½ÐµÑ‚ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ openpyxl Ð´Ð»Ñ .xlsx",
    "missing_dep_python_docx": "Ð½ÐµÑ‚ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ python-docx Ð´Ð»Ñ .docx",
    "missing_dep_pdfminer": "Ð½ÐµÑ‚ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ pdfminer.six Ð´Ð»Ñ .pdf",
    "unknown": "Ð¸Ð½Ð°Ñ Ð¿Ñ€Ð¸Ñ‡Ð¸Ð½Ð°",
}


_LAST_URLS: dict[int, str] = {}

_LIMITS_ATTR = "_page_limits"
_AWAIT_ATTR = "_await_page_limits"
_PENDING_DEEP_URLS_ATTR = "_pending_deep_urls"


def _get_limit_store(bot: Any) -> dict[int, int]:
    store = getattr(bot, _LIMITS_ATTR, None)
    if not isinstance(store, dict):
        store = {}
        setattr(bot, _LIMITS_ATTR, store)
    return store


def _get_awaiting_users(bot: Any) -> set[int]:
    waiting = getattr(bot, _AWAIT_ATTR, None)
    if not isinstance(waiting, set):
        waiting = set()
        setattr(bot, _AWAIT_ATTR, waiting)
    return waiting


def _get_pending_deep_urls(bot: Any) -> dict[int, str]:
    urls = getattr(bot, _PENDING_DEEP_URLS_ATTR, None)
    if not isinstance(urls, dict):
        urls = {}
        setattr(bot, _PENDING_DEEP_URLS_ATTR, urls)
    return urls


def _is_waiting_for_limit(message: types.Message) -> bool:
    user = message.from_user
    if user is None:
        return False
    waiting = _get_awaiting_users(message.bot)
    return user.id in waiting


def _normalize_page_limit(raw: Any) -> int:
    try:
        value = int(str(raw).strip())
    except Exception:
        value = 50
    if value < 1:
        value = 1
    if value > 500:
        value = 500
    return value


def _prepare_filtered(addresses: Iterable[str]) -> list[str]:
    return list(dict.fromkeys(_filter_stoplists(addresses)))


def _build_summary(
    filtered: list[str],
    stats: dict[str, int],
    *,
    deep: bool,
    limit_pages: int | None = None,
) -> str:
    summary = format_parse_summary(
        {
            "total_found": stats.get("total_in", 0),
            "to_send": len(filtered),
            "suspicious": 0,
            "cooldown_180d": 0,
            "foreign_domain": 0,
            "pages_skipped": 0,
            "footnote_dupes_removed": 0,
            "blocked": stats.get("blocked", 0),
            "blocked_after_parse": stats.get("blocked", 0),
        },
        examples=filtered[:5],
    )
    if filtered:
        summary += "\nÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹:\n" + "\n".join(hcode(addr) for addr in filtered[:5])
    pages = stats.get("pages", 0)
    if deep:
        used_limit = stats.get("pages_limit") or limit_pages
        if used_limit:
            summary += f"\n\nðŸŒ ÐŸÑ€Ð¾ÑÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†: {pages} (Ð»Ð¸Ð¼Ð¸Ñ‚ {used_limit})"
        elif pages:
            summary += f"\n\nðŸŒ ÐŸÑ€Ð¾ÑÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†: {pages}"
    return summary


def _format_rejects(rejects: dict[str, int], mapping: dict[str, str] | None = None) -> str:
    if not rejects:
        return ""
    mapping = mapping or REJECT_LABELS
    lines = ["\nÐŸÑ€Ð¸Ñ‡Ð¸Ð½Ñ‹ Ð¾Ñ‚Ð±Ñ€Ð°ÐºÐ¾Ð²ÐºÐ¸:"]
    for code, count in rejects.items():
        lines.append(f" â€¢ {mapping.get(code, code)} â€” {count}")
    return "\n".join(lines)


def _filter_stoplists(addresses: Iterable[str]) -> list[str]:
    return [email for email in addresses if not (is_blocked(email) or is_suppressed(email))]


@router.message(F.text & F.text.startswith("/ingest"))
async def handle_ingest(msg: types.Message) -> None:
    """Process `/ingest` command with newline separated addresses."""

    lines = [line for line in msg.text.splitlines()[1:] if line.strip()]
    ok, bad, stats = ingest_emails(lines)
    text = (
        f"Ð’ÑÐµÐ³Ð¾ ÑÑ‚Ñ€Ð¾Ðº: {stats['total_in']}\n"
        f"Ð“Ð¾Ð´Ð½Ñ‹Ñ… Ð°Ð´Ñ€ÐµÑÐ¾Ð²: {stats['ok']}\n"
        f"ÐžÑ‚Ð±Ñ€Ð¾ÑˆÐµÐ½Ð¾: {stats['bad']}"
    )
    rejects = stats.get("rejects") or {}
    text += _format_rejects(rejects)
    if ok:
        text += "\n\nÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹:\n" + "\n".join(hcode(x) for x in ok[:5])
    if bad:
        text += "\n\nÐžÑ‚Ð±Ñ€Ð¾ÑˆÐµÐ½Ð½Ñ‹Ðµ ÑÑ‚Ñ€Ð¾ÐºÐ¸:\n" + "\n".join(hcode(x) for x in bad[:5])
    await msg.answer(text)


@router.message(F.text.regexp(URL_RE))
async def handle_url(msg: types.Message) -> None:
    if not msg.text:
        return
    if msg.text.startswith("/ingest"):
        return
    match = URL_RE.search(msg.text)
    if not match:
        return
    url = match.group(1)
    url = url.rstrip(".,;:!?)]}")
    if not url.lower().startswith(("http://", "https://")):
        url = f"https://{url}"
    user_id = msg.from_user.id if msg.from_user else None
    if user_id is not None:
        _LAST_URLS[user_id] = url
    builder = InlineKeyboardBuilder()
    builder.button(text="ðŸ”Ž ÐŸÐ°Ñ€ÑÐ¸Ñ‚ÑŒ ÑÑ‚Ñƒ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ", callback_data="parse_url:single")
    builder.button(
        text="ðŸ•·ï¸ Ð¡ÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÑÐ°Ð¹Ñ‚",
        callback_data="parse_url:deep",
    )
    builder.adjust(1)
    await msg.answer(
        f"ÐÐ°ÑˆÐ»Ð° ÑÑÑ‹Ð»ÐºÑƒ:\n{hcode(url)}\nÐ’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ñ€ÐµÐ¶Ð¸Ð¼:",
        reply_markup=builder.as_markup(),
    )


async def _process_url_callback(
    callback: CallbackQuery,
    *,
    deep: bool,
    limit_pages: int | None = None,
    url: str | None = None,
) -> None:
    user_id = callback.from_user.id if callback.from_user else None
    if user_id is None:
        await callback.answer("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ñ‚ÑŒ ÑÑÑ‹Ð»ÐºÑƒ", show_alert=True)
        return
    pending_urls = _get_pending_deep_urls(callback.message.bot)
    if url is None:
        if deep:
            url = pending_urls.pop(user_id, None)
        if not url:
            url = _LAST_URLS.get(user_id)
    elif deep:
        pending_urls.pop(user_id, None)
    if not url:
        await callback.answer("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ñ‚ÑŒ ÑÑÑ‹Ð»ÐºÑƒ", show_alert=True)
        return
    waiting = _get_awaiting_users(callback.message.bot)
    waiting.discard(user_id)
    status_text = (
        f"ðŸ•·ï¸ Ð¡ÐºÐ°Ð½Ð¸Ñ€ÑƒÑŽ ÑÐ°Ð¹Ñ‚ (Ð»Ð¸Ð¼Ð¸Ñ‚ {limit_pages} ÑÑ‚Ñ€.):\n{hcode(url)}"
        if deep and limit_pages is not None
        else f"ðŸ•·ï¸ Ð¡ÐºÐ°Ð½Ð¸Ñ€ÑƒÑŽ ÑÐ°Ð¹Ñ‚:\n{hcode(url)}"
        if deep
        else f"ðŸ”Ž ÐŸÐ°Ñ€ÑÑŽ Ð¾Ð´Ð½Ñƒ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ:\n{hcode(url)}"
    )
    try:
        await callback.message.edit_text(status_text)
    except TelegramBadRequest:
        await callback.message.answer(status_text)
    if deep and limit_pages is not None:
        _get_limit_store(callback.message.bot)[user_id] = limit_pages
    try:
        ok, stats = await ingest_url(url, deep=deep, limit_pages=limit_pages)
    except Exception as exc:  # pragma: no cover - network errors are variable
        await callback.message.answer(
            f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ ÑÑÑ‹Ð»ÐºÑƒ {hcode(url)}: {exc}"
        )
        await callback.answer()
        return
    filtered = _prepare_filtered(ok)
    summary = _build_summary(filtered, stats, deep=deep, limit_pages=limit_pages)
    await callback.message.answer(summary)
    await callback.answer()


@router.callback_query(F.data == "parse_url:single")
async def parse_single(callback: CallbackQuery) -> None:
    await _process_url_callback(callback, deep=False)


@router.callback_query(F.data == "parse_url:deep")
async def parse_deep(callback: CallbackQuery) -> None:
    user_id = callback.from_user.id if callback.from_user else None
    if user_id is None:
        await callback.answer("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ñ‚ÑŒ ÑÑÑ‹Ð»ÐºÑƒ", show_alert=True)
        return
    url = _LAST_URLS.get(user_id)
    if not url:
        await callback.answer("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ñ‚ÑŒ ÑÑÑ‹Ð»ÐºÑƒ", show_alert=True)
        return
    pending_urls = _get_pending_deep_urls(callback.message.bot)
    pending_urls[user_id] = url
    keyboard = InlineKeyboardBuilder()
    for limit in (10, 25, 50, 100):
        keyboard.button(text=f"{limit} ÑÑ‚Ñ€.", callback_data=f"parse_limit:{limit}")
    keyboard.button(text="Ð”Ñ€ÑƒÐ³Ð¾Ðµâ€¦", callback_data="parse_limit:custom")
    keyboard.adjust(2)
    waiting = _get_awaiting_users(callback.message.bot)
    waiting.discard(user_id)
    text = (
        f"ðŸ•·ï¸ Ð¡ÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÑÐ°Ð¹Ñ‚Ð°:\n{hcode(url)}\n\nÐ’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ð»Ð¸Ð¼Ð¸Ñ‚ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†:"
    )
    try:
        await callback.message.edit_text(text, reply_markup=keyboard.as_markup())
    except TelegramBadRequest:
        await callback.message.answer(text, reply_markup=keyboard.as_markup())
    await callback.answer()


@router.callback_query(F.data.startswith("parse_limit:"))
async def parse_limit(callback: CallbackQuery) -> None:
    user_id = callback.from_user.id if callback.from_user else None
    if user_id is None:
        await callback.answer("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ñ‚ÑŒ ÑÑÑ‹Ð»ÐºÑƒ", show_alert=True)
        return
    pending_urls = _get_pending_deep_urls(callback.message.bot)
    url = pending_urls.get(user_id) or _LAST_URLS.get(user_id)
    if not url:
        await callback.answer("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ñ‚ÑŒ ÑÑÑ‹Ð»ÐºÑƒ", show_alert=True)
        return
    choice = callback.data.split("parse_limit:", 1)[1]
    waiting = _get_awaiting_users(callback.message.bot)
    if choice == "custom":
        waiting.add(user_id)
        prompt = "Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ð»Ð¸Ð¼Ð¸Ñ‚ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ† Ñ‡Ð¸ÑÐ»Ð¾Ð¼ (1â€“500):"
        try:
            await callback.message.edit_text(prompt)
        except TelegramBadRequest:
            await callback.message.answer(prompt)
        await callback.answer()
        return
    waiting.discard(user_id)
    limit = _normalize_page_limit(choice)
    await _process_url_callback(
        callback,
        deep=True,
        limit_pages=limit,
        url=pending_urls.get(user_id) or url,
    )


@router.message(F.text, F.func(_is_waiting_for_limit))
async def handle_limit_input(msg: types.Message) -> None:
    user_id = msg.from_user.id if msg.from_user else None
    if user_id is None:
        return
    waiting = _get_awaiting_users(msg.bot)
    if user_id not in waiting:
        return
    text = (msg.text or "").strip()
    if not text:
        await msg.answer("Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ð»Ð¸Ð¼Ð¸Ñ‚ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ† Ñ‡Ð¸ÑÐ»Ð¾Ð¼ (1â€“500).")
        return
    waiting.discard(user_id)
    pending_urls = _get_pending_deep_urls(msg.bot)
    url = pending_urls.pop(user_id, None) or _LAST_URLS.get(user_id)
    if not url:
        await msg.answer("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ñ‚ÑŒ ÑÑÑ‹Ð»ÐºÑƒ Ð´Ð»Ñ ÑÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ.")
        return
    limit = _normalize_page_limit(text)
    _get_limit_store(msg.bot)[user_id] = limit
    status_text = f"ðŸ•·ï¸ Ð¡ÐºÐ°Ð½Ð¸Ñ€ÑƒÑŽ ÑÐ°Ð¹Ñ‚ (Ð»Ð¸Ð¼Ð¸Ñ‚ {limit} ÑÑ‚Ñ€.):\n{hcode(url)}"
    await msg.answer(status_text)
    try:
        ok, stats = await ingest_url(url, deep=True, limit_pages=limit)
    except Exception as exc:  # pragma: no cover - network errors are variable
        await msg.answer(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ ÑÑÑ‹Ð»ÐºÑƒ {hcode(url)}: {exc}")
        return
    filtered = _prepare_filtered(ok)
    summary = _build_summary(filtered, stats, deep=True, limit_pages=limit)
    await msg.answer(summary)


@router.message(F.document)
async def handle_document(msg: types.Message) -> None:
    doc = msg.document
    ack = await msg.reply(f"ÐŸÑ€Ð¸Ð½ÑÐ»Ð° Ñ„Ð°Ð¹Ð»: {doc.file_name}. ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÑŽâ€¦")
    buffer = io.BytesIO()
    await msg.bot.download(doc, destination=buffer)
    data = buffer.getvalue()
    try:
        ok, rejects, warn = extract_emails_from_bytes(data, doc.file_name or "file")
    except ExtractError as exc:
        await ack.edit_text(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ñ„Ð°Ð¹Ð»: {exc}")
        return
    except Exception:  # pragma: no cover - unexpected decoding errors
        await ack.edit_text("ÐŸÑ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ñ€Ð°Ð·Ð±Ð¾Ñ€Ðµ Ñ„Ð°Ð¹Ð»Ð°.")
        return

    ok = list(dict.fromkeys(_filter_stoplists(ok)))
    text = f"Ð“Ð¾Ñ‚Ð¾Ð²Ð¾.\nÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ð°Ð´Ñ€ÐµÑÐ¾Ð²: {len(ok)}"
    text += _format_rejects(rejects)
    if warn:
        text += f"\n\nâš ï¸ {warn}"
    if ok:
        text += "\n\nÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹:\n" + "\n".join(hcode(x) for x in ok[:5])
    await ack.edit_text(text)


@router.callback_query(F.data.startswith("set_group:"))
async def set_group(callback: CallbackQuery) -> None:
    """Handle group selection from inline keyboard."""

    label = callback.data.split("set_group:", 1)[1]
    slug = resolve_label(label)
    await callback.message.answer(f"Ð’Ñ‹ Ð²Ñ‹Ð±Ñ€Ð°Ð»Ð¸: {label} ({slug})")
    await callback.answer()
